# DC Bikeshare Analytics Pipeline

A production-grade data pipeline that ingests, transforms, and analyzes Washington DC bikeshare trip data using Apache Spark, Delta Lake, and Databricks Asset Bundles (DAB). The pipeline implements a medallion architecture (bronze/silver/gold) with full CI/CD automation and multi-environment deployment.

---

## ğŸ“Š Project Overview

This project processes one year of Washington DC bikeshare trip metadata, tracking:

- Start and end station information
- Trip duration and distance calculations
- Rider type classification (member vs casual)
- Temporal patterns (day of week, time of day)
- Geographic patterns (station popularity, route analysis)

Data flows from AWS S3 through Databricks using Spark Structured Streaming, transformed via medallion architecture, and stored as Delta tables for analytics.

---

## ğŸ—ï¸ Architecture

### Multi-Environment Setup

| Environment | Region    | Workspace           | Catalog             | S3 Bucket                        | Purpose                   |
| ----------- | --------- | ------------------- | ------------------- | -------------------------------- | ------------------------- |
| **Dev**     | us-west-1 | `dbc-b4813f44-2c67` | `dev_bikeshare`     | `s3://dc-bikeshare-data-dev`     | Development and testing   |
| **Staging** | us-east-1 | `dbc-b9fdb8d4-ebaa` | `staging_bikeshare` | `s3://dc-bikeshare-data-staging` | Pre-production validation |
| **Prod**    | us-east-1 | `dbc-1886d4dd-39a5` | `prod_bikeshare`    | `s3://dc-bikeshare-data`         | Production analytics      |

### Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Raw S3 Data                            â”‚
â”‚              s3://{env}-bikeshare/raw/                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER (Raw Ingestion)                               â”‚
â”‚  - Auto Loader (Spark Structured Streaming)                 â”‚
â”‚  - Schema inference and evolution                           â”‚
â”‚  - CSV â†’ Delta Lake conversion                              â”‚
â”‚  Table: {catalog}.bronze.dc_rideshare_bt                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER (Cleansed & Enriched)                         â”‚
â”‚  - Data quality checks                                      â”‚
â”‚  - Trip distance calculation (Haversine formula)            â”‚
â”‚  - Duration computation                                     â”‚
â”‚  - Trip type classification (round trip vs one-way)         â”‚
â”‚  - Temporal features (day of week, hour)                    â”‚
â”‚  Table: {catalog}.silver.dc_rideshare_cleaned               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD LAYER (Analytics-Ready Aggregations)                  â”‚
â”‚  - Station popularity metrics                               â”‚
â”‚  - Temporal usage patterns                                  â”‚
â”‚  - Route analysis                                           â”‚
â”‚  - Member vs casual comparisons                             â”‚
â”‚  Tables: {catalog}.gold.fact_rides_summary                  â”‚
â”‚          {catalog}.gold.station_metrics                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### IAM & Security

**Storage Credentials (Unity Catalog):**

- Each environment has its own AWS IAM role
- Separate storage credentials per environment:
  - `dev-bikeshare-storage-cred` â†’ IAM role for dev bucket access
  - `staging-bikeshare-storage-cred` â†’ IAM role for staging bucket access
  - `prod-bikeshare-storage-cred` â†’ IAM role for prod bucket access
- External locations defined per environment for governed data access

**S3 Access Pattern:**

```
Unity Catalog External Location
    â†“ (uses)
Storage Credential (IAM Role)
    â†“ (grants access to)
S3 Bucket (environment-specific)
```

---

## ğŸ› ï¸ Technology Stack

### Core Technologies

- **Data Processing:** Apache Spark 3.5.0 (Databricks Runtime 14.3 LTS)
- **Storage Format:** Delta Lake 3.0+
- **Orchestration:** Databricks Asset Bundles (DAB) 0.277.0
- **Data Governance:** Unity Catalog
- **Cloud Storage:** AWS S3
- **Streaming:** Spark Structured Streaming with Auto Loader

### Development & CI/CD

- **Language:** Python 3.11
- **Testing:** pytest 7.4.3, PySpark 3.5.0
- **Version Control:** Git/GitHub
- **CI/CD:** GitHub Actions
- **CLI:** Databricks CLI 0.277.0

### Infrastructure as Code

- **Deployment:** Databricks Asset Bundles (YAML)
- **Parameterization:** dbutils.widgets + DAB base_parameters
- **Environment Management:** Separate targets (dev/staging/prod)

---

## ğŸ“ Project Structure

```
dc-bikeshare/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ test.yml                 # Unit tests on PR
â”‚       â”œâ”€â”€ deploy-dev.yml           # Auto-deploy to dev
â”‚       â”œâ”€â”€ deploy-staging.yml       # Auto-deploy to staging (after dev)
â”‚       â””â”€â”€ deploy-prod.yml          # Manual approval deploy to prod
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bikeshare_bronze_nb.py       # Bronze layer ingestion
â”‚   â”œâ”€â”€ bikeshare_silver_nb.py       # Silver layer transformations
â”‚   â””â”€â”€ bikeshare_gold_nb.py         # Gold layer aggregations
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                  # Pytest fixtures (local Spark)
â”‚   â””â”€â”€ test_transformations.py      # Unit tests
â”œâ”€â”€ databricks.yml                   # DAB configuration
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ CI/CD Pipeline

### Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Developer Workflow                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Create Feature Branch
   git checkout -b feature/new-transformation

2. Make Changes & Test Locally
   pytest tests/ -v

3. Commit & Push
   git push origin feature/new-transformation

4. Open Pull Request to 'dev'
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions: Run Tests                                   â”‚
â”‚  - Lint code                                                 â”‚
â”‚  - Run pytest unit tests                                     â”‚
â”‚  - Validate DAB configuration                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“ (tests pass)

5. Merge PR to 'dev' branch
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions: Deploy to Dev                               â”‚
â”‚  - Install Databricks CLI                                    â”‚
â”‚  - Authenticate to dev workspace                             â”‚
â”‚  - Run: databricks bundle deploy --target dev                â”‚
â”‚  - Execute pipeline: databricks bundle run bikeshare_etl     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“ (dev succeeds)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions: Deploy to Staging (Sequential)              â”‚
â”‚  - Triggered by dev workflow completion                      â”‚
â”‚  - Deploy to staging workspace                               â”‚
â”‚  - Run integration tests                                     â”‚
â”‚  - Validate data quality                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“ (staging succeeds)

6. Open Pull Request from 'dev' â†’ 'main'
   - Code review required
   - Approval from team lead

7. Merge to 'main' branch
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions: Deploy to Production                        â”‚
â”‚  - Workflow pauses for manual approval â¸ï¸                   â”‚
â”‚  - Requires approval from: green.leek47@gmail.com            â”‚
â”‚  - After approval: deploy to prod workspace                  â”‚
â”‚  - Execute production pipeline                               â”‚
â”‚  - Send success notifications                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Gates

- **Dev:** Automatic on merge to dev branch
- **Staging:** Automatic after dev succeeds
- **Production:** Requires manual approval via GitHub Environment

---

## ğŸ§ª Testing Strategy

### Unit Tests (Local)

```bash
# Run all tests
pytest tests/ -v

# Run specific test
pytest tests/test_transformations.py::test_trip_type_classification -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

**Test Coverage:**

- Trip type classification (round trip vs one-way)
- Data quality flag logic (suspicious durations)
- Haversine distance calculations
- Temporal feature extraction (weekend detection)

### Integration Tests (Staging)

- End-to-end pipeline execution
- Data quality assertions on silver/gold tables
- Cross-layer consistency checks
- Performance benchmarks

---

### Project dependencies

- Python 3.11+
- Databricks CLI 0.277.0+
- AWS credentials (for S3 access)
- Databricks workspace access

---

## ğŸ”§ Configuration

### Environment-Specific Variables

All environment-specific configuration is defined in `databricks.yml`:

```yaml
targets:
  dev:
    mode: development
    workspace:
      host: https://dbc-b4813f44-2c67.cloud.databricks.com
    variables:
      catalog: dev_bikeshare
      source_bucket: s3://dc-bikeshare-data-dev

  staging:
    mode: production
    workspace:
      host: https://dbc-b9fdb8d4-ebaa.cloud.databricks.com
      root_path: /Workspace/Users/green.malik5@gmail.com/.bundle/${bundle.name}/${bundle.target}
    variables:
      catalog: staging_bikeshare
      source_bucket: s3://dc-bikeshare-data-staging

  prod:
    mode: production
    workspace:
      host: https://dbc-1886d4dd-39a5.cloud.databricks.com
      root_path: /Workspace/Users/green.malik5@gmail.com/.bundle/${bundle.name}/${bundle.target}
    variables:
      catalog: prod_bikeshare
      source_bucket: s3://dc-bikeshare-data
```

### Notebook Parameterization

All notebooks use `dbutils.widgets` for parameterization:

```python
# Parameters - overridden by DAB base_parameters
dbutils.widgets.text("catalog", "OVERRIDE_ME")
dbutils.widgets.text("bronze_schema", "bronze")
dbutils.widgets.text("source_bucket", "s3://OVERRIDE_ME")

# Retrieved at runtime
catalog = dbutils.widgets.get("catalog")
bronze_schema = dbutils.widgets.get("bronze_schema")
source_bucket = dbutils.widgets.get("source_bucket")
```

DAB injects environment-specific values via `base_parameters` in the job definition.

---

## ğŸš¨ Current Challenges & Known Issues

### 1. DAB Sync Cache Corruption (Critical - Unresolved)

**Symptom:**

- DAB persistently deploys stale notebook versions despite file changes
- Cache clearing procedures ineffective:
  - `databricks bundle destroy` doesn't fix it
  - Deleting local `.databricks/` cache doesn't fix it
  - Deleting remote `.bundle/` workspace directory doesn't fix it
  - Deleting `sync-snapshots/` metadata doesn't fix it

**Observed Behavior:**

- Local files are updated and committed
- GitHub repository shows updated code
- After deployment, old non-parameterized notebooks still appear in workspace
- DAB's sync mechanism incorrectly reports "no changes detected"

**Current Workaround:**

- Rename notebook files (e.g., `bikeshare_silver.py` â†’ `bikeshare_silver_nb.py`)
- DAB treats renamed files as "new" and uploads them correctly
- **Limitation:** This is not viable if file naming conventions are strict

**Root Cause:**

- Unknown. Likely a bug in DAB's file sync mechanism or workspace-level caching
- Appears to persist across DAB versions and workspace configurations
- May be related to timestamp comparison vs content hashing

**Impact:**

- Risk of deploying stale code in production if cache corruption occurs
- No reliable fix exists for forcing re-upload without renaming files
- Requires vigilance during deployments to verify correct versions are deployed

**Potential Mitigations:**

1. Always verify deployed notebook content in workspace UI after deployment
2. Include content checksums in deployment logs for verification
3. Consider scripted cache clearing before critical deployments:
   ```bash
   rm -rf .databricks/
   databricks workspace delete /Workspace/Users/{user}/.bundle/{bundle} --recursive
   databricks bundle deploy --target prod
   ```
4. File a detailed bug report with Databricks support

**Status:** Open issue. Workaround functional but not ideal. Investigating further.

---

### 2. Production Mode Root Path Requirement

**Issue:**
When using `mode: production` in DAB targets, `workspace.root_path` must be explicitly set.

**Error Message:**

```
Error: target with 'mode: production' must set 'workspace.root_path' to make sure only one copy is deployed
```

**Resolution:**
Add `root_path` to all production targets:

```yaml
staging:
  mode: production
  workspace:
    root_path: /Workspace/Users/green.malik5@gmail.com/.bundle/${bundle.name}/${bundle.target}
```

**Design Rationale:**

- Prevents multiple users/CI systems from overwriting each other's deployments
- Enforces explicit path declaration for production safety
- Development mode is more permissive, production mode has guardrails

---

## ğŸ“ˆ Future Enhancements

- [ ] Add Great Expectations for comprehensive data quality checks
- [ ] Implement job scheduling (daily at 2 AM ET)
- [ ] Add Slack/Teams notifications for job failures
- [ ] Create data quality dashboard in Databricks SQL
- [ ] Implement incremental processing for cost optimization
- [ ] Add rollback capability with Git tags
- [ ] Expand gold layer with additional analytics tables
- [ ] Add performance benchmarking tests

---

## ğŸ“ Contributing

1. Create a feature branch from `dev`
2. Make changes and add tests
3. Ensure all tests pass locally: `pytest tests/ -v`
4. Open a pull request to `dev` branch
5. Wait for automated tests to pass
6. Request code review
7. After approval, merge to `dev` (auto-deploys to dev & staging)
8. For production release, create PR from `dev` to `main`

---

## ğŸ“„ License

This is a portfolio project for demonstration purposes. Please contact the maintainer for usage permissions.

---

## ğŸ™ Acknowledgments

- Built with [Databricks Asset Bundles](https://docs.databricks.com/dev-tools/bundles/)
- Uses [Unity Catalog](https://www.databricks.com/product/unity-catalog) for data governance
- CI/CD powered by [GitHub Actions](https://github.com/features/actions)
- Data source: [Capital Bikeshare System Data](https://capitalbikeshare.com/system-data)

---

## ğŸ“ Contact

**Maintainer:** Malik Green  
**Email:** green.malik5@gmail.com  
**GitHub:** [@MalikCoderGreen](https://github.com/MalikCoderGreen)

---

**Last Updated:** February 2026  
**DAB Version:** 0.277.0  
**Python Version:** 3.11  
**Spark Version:** 3.5.0 (Databricks Runtime 14.3 LTS)
